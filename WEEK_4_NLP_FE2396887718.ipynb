{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvJcTG/M8a1o1R28T2WGrY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/techylimat/techylimat/blob/main/WEEK_4_NLP_FE2396887718.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Applied Learning Assignment 1"
      ],
      "metadata": {
        "id": "RWnWtiF9ueg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Natural Language Processing (NLP) is a branch of artificial intelligence that enables computers to understand, interprete, and generate human language. By combining computational linguistics with machine learning techniques, NLP allows machines to process text and speech in a way that is both meaningful and useful."
      ],
      "metadata": {
        "id": "jVS3UylMu1yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\n",
        "1. Clinical Documentation in Healthcare\n",
        "\n",
        "NLP is employed to extract and structure information from unstructured clinical notes and medical records. This streamlines the documentation process, reduces administrative burdens on healthcare professionals, and enhances the accuracy of Electronic Health Records (EHRs). Improved documentation leads to better patient care and more efficient healthcare delivery.\n",
        "2. Risk Assessment in Finance\n",
        "\n",
        "Financial institutions utilize NLP to analyze unstructured data, such as news articles and reports, to assess potential risks. By processing this information, NLP helps in identifying market trends, evaluating credit risk, and making informed investment decisions, thereby enhancing financial stability and decision-making processes.\n",
        "\n",
        "3. Email Filtering Systems\n",
        "\n",
        "NLP powers email filters that automatically categorize incoming emails into folders like Primary, Social, and Promotions. By understanding the content and context of messages, these filters help users manage their inboxes more effectively, reducing clutter and highlighting important communications.\n",
        "\n"
      ],
      "metadata": {
        "id": "zuXsdq4cwZTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Sarcasm and Irony Detection\n",
        "\n",
        "NLP systems often struggle to identify sarcasm and irony, as these rely on subtle cues and context that are difficult for machines to discern. Misinterpreting such expressions can lead to incorrect sentiment analysis and misunderstandings in human-computer interactions. ​\n",
        "\n",
        "2. Data Scarcity in Low-Resource Languages\n",
        "\n",
        "Many languages lack extensive digital corpora, making it challenging to develop effective NLP models for them. This data scarcity hampers the creation of language tools and limits the reach of NLP applications globally."
      ],
      "metadata": {
        "id": "hKvpLaBZxioQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4a\n",
        "import re\n",
        "\n",
        "text = \"Contact us at support@company.com or sales@business.org. For more, email info@service.net.\"\n",
        "\n",
        "# Regular expression pattern for matching email addresses\n",
        "email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "\n",
        "# Find all matches in the text\n",
        "emails = re.findall(email_pattern, text)\n",
        "\n",
        "print(emails)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lel31SQUuIGN",
        "outputId": "9774ede8-fe4a-497d-e54b-38cd0ee54b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['support@company.com', 'sales@business.org', 'info@service.net']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKh1GSATtbwH",
        "outputId": "ea4d9b04-ca52-47c5-cc00-5f0f9801b70a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['amazing', 'cleaning', 'processing', 'learning']\n"
          ]
        }
      ],
      "source": [
        "#4b\n",
        "\n",
        "import re\n",
        "\n",
        "sentence = \"NLP is amazing for cleaning and processing text while learning new techniques.\"\n",
        "\n",
        "# Regular expression pattern to match words ending with 'ing'\n",
        "pattern = r'\\b\\w+ing\\b'\n",
        "\n",
        "# Find all matching words\n",
        "ing_words = re.findall(pattern, sentence)\n",
        "\n",
        "print(ing_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "import string\n",
        "\n",
        "# Original text\n",
        "text = \"NLP makes AI smarter! But, sometimes, it’s challenging… Don’t you agree?\"\n",
        "\n",
        "# Remove punctuation\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "text_no_punct = text.translate(translator)\n",
        "\n",
        "# Convert to lowercase\n",
        "text_lower = text_no_punct.lower()\n",
        "\n",
        "# Split into words\n",
        "words = text_lower.split()\n",
        "\n",
        "# Output the processed words\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtoMvYcEYNr0",
        "outputId": "413e8247-bf59-4d36-f229-c8979fedbe35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nlp', 'makes', 'ai', 'smarter', 'but', 'sometimes', 'it’s', 'challenging…', 'don’t', 'you', 'agree']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Applied Learning Assignment 2"
      ],
      "metadata": {
        "id": "w-gacRgDafM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "\n",
        "import string\n",
        "import re\n",
        "\n",
        "text = \"OMG!! NLP is soooo coool 🤩...!!! It costs $1000. Learn it now at https://3mtt.com 😎\"\n",
        "\n",
        "# Convert to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Remove punctuation\n",
        "text = ''.join([char for char in text if char not in string.punctuation])\n",
        "\n",
        "# Remove numbers\n",
        "text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "# Remove URLs\n",
        "text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "# Remove emojis and other special characters\n",
        "text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzrswPAnRsjw",
        "outputId": "b0b66295-8462-47df-e3d8-d1687c94673a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "omg nlp is soooo coool  it costs  learn it now at  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text = \"Tokenization is the first step in NLP. It splits text into smaller pieces for analysis.\"\n",
        "\n",
        "# Sentence-level tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence-level Tokenization:\")\n",
        "print(sentences)\n",
        "\n",
        "# Word-level tokenization\n",
        "words = word_tokenize(text)\n",
        "print(\"\\nWord-level Tokenization:\")\n",
        "print(words)"
      ],
      "metadata": {
        "id": "2MHfvI695_kN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be999f9-1a28-4d33-ce3c-486850c9715f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence-level Tokenization:\n",
            "['Tokenization is the first step in NLP.', 'It splits text into smaller pieces for analysis.']\n",
            "\n",
            "Word-level Tokenization:\n",
            "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP', '.', 'It', 'splits', 'text', 'into', 'smaller', 'pieces', 'for', 'analysis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "# Initialize spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Words to process\n",
        "words = [\"running\", \"flies\", \"studies\", \"easily\", \"studying\", \"better\"]\n",
        "\n",
        "# Stemming using Porter Stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(\"Stemmed words:\", stemmed_words)\n",
        "\n",
        "# Lemmatization using spaCy\n",
        "doc = nlp(\" \".join(words))  # Create a spaCy Doc object\n",
        "lemmatized_words = [token.lemma_ for token in doc]\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "id": "jJHfTPQt6DCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b82d39a8-4382-45ff-dc42-fbf162de8772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Stemmed words: ['run', 'fli', 'studi', 'easili', 'studi', 'better']\n",
            "Lemmatized words: ['run', 'fly', 'study', 'easily', 'study', 'well']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Applied Learning Assignment 3"
      ],
      "metadata": {
        "id": "G2HUlX_VKKkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define vocabulary\n",
        "vocabulary = [\"ink\", \"book\", \"dictionary\", \"pencil\", \"eraser\"]\n",
        "\n",
        "# Create a dictionary to map words to indices\n",
        "word_to_index = {word: index for index, word in enumerate(vocabulary)}\n",
        "\n",
        "def one_hot_encode(word):\n",
        "  \"\"\"Generates a one-hot encoded vector for a given word.\n",
        "\n",
        "  Args:\n",
        "    word: The word to encode.\n",
        "\n",
        "  Returns:\n",
        "    A one-hot encoded NumPy array.\n",
        "  \"\"\"\n",
        "  vector = np.zeros(len(vocabulary), dtype=int)\n",
        "  if word in word_to_index:\n",
        "    index = word_to_index[word]\n",
        "    vector[index] = 1\n",
        "  return vector\n",
        "\n",
        "# Example usage\n",
        "word = \"dictionary\"\n",
        "encoded_vector = one_hot_encode(word)\n",
        "print(f\"One-hot encoded vector for '{word}': {encoded_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32QxNaE6KRCt",
        "outputId": "def2ce1b-97c4-46d4-e49e-231d69d56071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot encoded vector for 'dictionary': [0 0 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Sample dataset\n",
        "dataset = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"the dog sleeps in the kernel\"\n",
        "]\n",
        "\n",
        "# Bag of Words representation using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(dataset)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"Bag of Words Representation:\")\n",
        "print(bow_matrix.toarray())\n",
        "print(\"Feature Names:\", feature_names)\n",
        "\n",
        "# TF-IDF representation using TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(dataset)\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"\\nTF-IDF Representation:\")\n",
        "print(tfidf_matrix.toarray())\n",
        "print(\"Feature Names:\", tfidf_feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nex9XnHUMlZS",
        "outputId": "73799f2c-3b20-481b-c5cf-725b4ff12cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Representation:\n",
            "[[1 1 1 0 1 0 1 1 1 0 2]\n",
            " [0 1 0 1 0 1 0 0 0 1 2]]\n",
            "Feature Names: ['brown' 'dog' 'fox' 'in' 'jumps' 'kernel' 'lazy' 'over' 'quick' 'sleeps'\n",
            " 'the']\n",
            "\n",
            "TF-IDF Representation:\n",
            "[[0.342369   0.24359836 0.342369   0.         0.342369   0.\n",
            "  0.342369   0.342369   0.342369   0.         0.48719673]\n",
            " [0.         0.30253071 0.         0.42519636 0.         0.42519636\n",
            "  0.         0.         0.         0.42519636 0.60506143]]\n",
            "Feature Names: ['brown' 'dog' 'fox' 'in' 'jumps' 'kernel' 'lazy' 'over' 'quick' 'sleeps'\n",
            " 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "\n",
        "!pip install --force-reinstall --no-cache-dir gensim \"numpy<2.0,>=1.18.5\" \"scipy<1.14.0,>=1.7.0\"\n",
        "!pip install gensim\n",
        "\n",
        "import gensim\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "\n",
        "# Sample dataset of sentences\n",
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The dog barked at the mailman.\",\n",
        "    \"The fox and the dog are friends.\"\n",
        "]\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = gensim.models.Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
        "# vector_size: Dimensionality of word vectors (embedding size)\n",
        "# window: Maximum distance between the current and predicted word within a sentence\n",
        "# min_count: Ignores all words with total frequency lower than this\n",
        "# sg: Training algorithm (0 for CBOW, 1 for skip-gram)\n",
        "\n",
        "\n",
        "# Retrieve the embedding for \"dog\"\n",
        "dog_embedding = model.wv['dog']\n",
        "print(\"Embedding for 'dog':\", dog_embedding)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlReqk6INFyB",
        "outputId": "60859b33-c750-4f01-84d2-6743b8a850af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m139.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open>=1.8.1 (from gensim)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m174.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m188.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m139.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m235.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m230.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Embedding for 'dog': [ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
            "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
            " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
            "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
            "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
            " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
            "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
            " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
            " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
            " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
            " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
            "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
            " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
            " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
            " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
            "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
            " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
            " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
            "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
            "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
            "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
            "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
            "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
            "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
            "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load the pre-trained GloVe model\n",
        "model = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "# Retrieve the embedding for the word \"king\"\n",
        "king_embedding = model['king']\n",
        "print(f\"Embedding for 'king': {king_embedding}\")\n",
        "\n",
        "# Find the 5 most similar words to \"king\"\n",
        "similar_words = model.most_similar('king', topn=5)\n",
        "print(f\"5 most similar words to 'king': {similar_words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HbDpIOfQ1vE",
        "outputId": "09d9a5ea-de6b-4994-c227-c1ba86b574ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "Embedding for 'king': [ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n",
            "  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n",
            "  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n",
            " -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n",
            " -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n",
            "  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n",
            " -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n",
            " -0.51042 ]\n",
            "5 most similar words to 'king': [('prince', 0.8236179351806641), ('queen', 0.7839043140411377), ('ii', 0.7746230363845825), ('emperor', 0.7736247777938843), ('son', 0.766719400882721)]\n"
          ]
        }
      ]
    }
  ]
}